[INFO] 2019-01-23 21:30:37, pid=47743, src=inference.py:54, Enable inference model done.
[INFO] 2019-01-23 21:30:37, pid=47743, src=server.py:142, server start, port=6008
[WARNING] 2019-01-23 21:31:58, pid=47743, src=server.py:93, Caught signal: 2
[INFO] 2019-01-23 21:31:58, pid=47743, src=server.py:98, begin to stop http server ...
[INFO] 2019-01-23 21:31:58, pid=47743, src=server.py:101, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 21:31:59, pid=47743, src=server.py:93, Caught signal: 2
[INFO] 2019-01-23 21:31:59, pid=47743, src=server.py:98, begin to stop http server ...
[INFO] 2019-01-23 21:31:59, pid=47743, src=server.py:101, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 21:31:59, pid=47743, src=server.py:93, Caught signal: 2
[INFO] 2019-01-23 21:31:59, pid=47743, src=server.py:98, begin to stop http server ...
[INFO] 2019-01-23 21:31:59, pid=47743, src=server.py:101, shutdown in 0.5 seconds ...
[INFO] 2019-01-23 21:31:59, pid=47743, src=server.py:111, shutdown finished
[INFO] 2019-01-23 21:33:11, pid=48413, src=inference.py:54, Enable inference model done.
[INFO] 2019-01-23 21:33:11, pid=48413, src=server.py:143, server start, port=6008
[WARNING] 2019-01-23 21:34:09, pid=48413, src=server.py:94, Caught signal: 2
[INFO] 2019-01-23 21:34:09, pid=48413, src=server.py:99, begin to stop http server ...
[INFO] 2019-01-23 21:34:09, pid=48413, src=server.py:102, shutdown in 0.5 seconds ...
[INFO] 2019-01-23 21:34:10, pid=48413, src=server.py:112, shutdown finished
[INFO] 2019-01-23 21:34:17, pid=48834, src=inference.py:54, Enable inference model done.
[INFO] 2019-01-23 21:34:17, pid=48834, src=server.py:143, server start, port=6008
[WARNING] 2019-01-23 21:38:50, pid=48834, src=server.py:94, Caught signal: 2
[INFO] 2019-01-23 21:38:50, pid=48834, src=server.py:99, begin to stop http server ...
[INFO] 2019-01-23 21:38:50, pid=48834, src=server.py:102, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 21:38:50, pid=48834, src=server.py:94, Caught signal: 2
[INFO] 2019-01-23 21:38:50, pid=48834, src=server.py:99, begin to stop http server ...
[INFO] 2019-01-23 21:38:50, pid=48834, src=server.py:102, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 21:38:51, pid=48834, src=server.py:94, Caught signal: 2
[INFO] 2019-01-23 21:38:51, pid=48834, src=server.py:99, begin to stop http server ...
[INFO] 2019-01-23 21:38:51, pid=48834, src=server.py:102, shutdown in 0.5 seconds ...
[INFO] 2019-01-23 21:38:51, pid=48834, src=server.py:112, shutdown finished
[INFO] 2019-01-23 21:38:59, pid=49912, src=inference.py:54, Enable inference model done.
[INFO] 2019-01-23 21:38:59, pid=49912, src=server.py:153, server start, port=6008
[WARNING] 2019-01-23 21:42:21, pid=49912, src=server.py:103, Caught signal: 2
[INFO] 2019-01-23 21:42:21, pid=49912, src=server.py:108, begin to stop http server ...
[INFO] 2019-01-23 21:42:21, pid=49912, src=server.py:111, shutdown in 0.5 seconds ...
[INFO] 2019-01-23 21:42:21, pid=49912, src=server.py:121, shutdown finished
[INFO] 2019-01-23 21:42:42, pid=50793, src=inference.py:54, Enable inference model done.
[INFO] 2019-01-23 21:42:42, pid=50793, src=server.py:153, server start, port=6008
[WARNING] 2019-01-23 21:43:09, pid=50793, src=server.py:103, Caught signal: 2
[INFO] 2019-01-23 21:43:09, pid=50793, src=server.py:108, begin to stop http server ...
[INFO] 2019-01-23 21:43:09, pid=50793, src=server.py:111, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 21:43:09, pid=50793, src=server.py:103, Caught signal: 2
[INFO] 2019-01-23 21:43:09, pid=50793, src=server.py:108, begin to stop http server ...
[INFO] 2019-01-23 21:43:09, pid=50793, src=server.py:111, shutdown in 0.5 seconds ...
[INFO] 2019-01-23 21:43:10, pid=50793, src=server.py:121, shutdown finished
[INFO] 2019-01-23 21:43:18, pid=51089, src=inference.py:54, Enable inference model done.
[INFO] 2019-01-23 21:43:18, pid=51089, src=server.py:154, server start, port=6008
[WARNING] 2019-01-23 21:45:15, pid=51089, src=server.py:104, Caught signal: 2
[INFO] 2019-01-23 21:45:15, pid=51089, src=server.py:109, begin to stop http server ...
[INFO] 2019-01-23 21:45:15, pid=51089, src=server.py:112, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 21:45:16, pid=51089, src=server.py:104, Caught signal: 2
[INFO] 2019-01-23 21:45:16, pid=51089, src=server.py:109, begin to stop http server ...
[INFO] 2019-01-23 21:45:16, pid=51089, src=server.py:112, shutdown in 0.5 seconds ...
[INFO] 2019-01-23 21:45:16, pid=51089, src=server.py:122, shutdown finished
[INFO] 2019-01-23 21:45:24, pid=51685, src=inference.py:54, Enable inference model done.
[INFO] 2019-01-23 21:45:24, pid=51685, src=server.py:154, server start, port=6008
[WARNING] 2019-01-23 21:49:14, pid=51685, src=server.py:104, Caught signal: 2
[INFO] 2019-01-23 21:49:14, pid=51685, src=server.py:109, begin to stop http server ...
[INFO] 2019-01-23 21:49:14, pid=51685, src=server.py:112, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 21:49:14, pid=51685, src=server.py:104, Caught signal: 2
[INFO] 2019-01-23 21:49:14, pid=51685, src=server.py:109, begin to stop http server ...
[INFO] 2019-01-23 21:49:14, pid=51685, src=server.py:112, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 21:49:15, pid=51685, src=server.py:104, Caught signal: 2
[INFO] 2019-01-23 21:49:15, pid=51685, src=server.py:109, begin to stop http server ...
[INFO] 2019-01-23 21:49:15, pid=51685, src=server.py:112, shutdown in 0.5 seconds ...
[INFO] 2019-01-23 21:49:15, pid=51685, src=server.py:122, shutdown finished
[INFO] 2019-01-23 21:49:26, pid=52608, src=inference.py:54, Enable inference model done.
[INFO] 2019-01-23 21:49:26, pid=52608, src=server.py:163, server start, port=6008
[WARNING] 2019-01-23 22:09:45, pid=52608, src=server.py:113, Caught signal: 2
[INFO] 2019-01-23 22:09:45, pid=52608, src=server.py:118, begin to stop http server ...
[INFO] 2019-01-23 22:09:45, pid=52608, src=server.py:121, shutdown in 0.5 seconds ...
[INFO] 2019-01-23 22:09:45, pid=52608, src=server.py:131, shutdown finished
[INFO] 2019-01-23 22:09:53, pid=57267, src=inference.py:54, Enable inference model done.
[INFO] 2019-01-23 22:10:38, pid=584, src=inference.py:54, Enable inference model done.
[INFO] 2019-01-23 22:10:38, pid=584, src=server.py:111, server start, port=6008
[WARNING] 2019-01-23 22:12:02, pid=584, src=server.py:62, Caught signal: 2
[INFO] 2019-01-23 22:12:02, pid=584, src=server.py:67, begin to stop http server ...
[INFO] 2019-01-23 22:12:02, pid=584, src=server.py:70, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 22:12:02, pid=584, src=server.py:62, Caught signal: 2
[INFO] 2019-01-23 22:12:02, pid=584, src=server.py:67, begin to stop http server ...
[INFO] 2019-01-23 22:12:02, pid=584, src=server.py:70, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 22:12:02, pid=584, src=server.py:62, Caught signal: 2
[INFO] 2019-01-23 22:12:02, pid=584, src=server.py:67, begin to stop http server ...
[INFO] 2019-01-23 22:12:02, pid=584, src=server.py:70, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 22:12:02, pid=584, src=server.py:62, Caught signal: 2
[INFO] 2019-01-23 22:12:02, pid=584, src=server.py:67, begin to stop http server ...
[INFO] 2019-01-23 22:12:02, pid=584, src=server.py:70, shutdown in 0.5 seconds ...
[INFO] 2019-01-23 22:12:03, pid=584, src=server.py:80, shutdown finished
[INFO] 2019-01-23 22:12:11, pid=1203, src=inference.py:54, Enable inference model done.
[INFO] 2019-01-23 22:12:11, pid=1203, src=server.py:114, server start, port=6008
[WARNING] 2019-01-23 22:12:22, pid=1203, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:12:22, pid=1203, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:12:22, pid=1203, src=server.py:73, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 22:12:23, pid=1203, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:12:23, pid=1203, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:12:23, pid=1203, src=server.py:73, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 22:12:23, pid=1203, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:12:23, pid=1203, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:12:23, pid=1203, src=server.py:73, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 22:12:23, pid=1203, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:12:23, pid=1203, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:12:23, pid=1203, src=server.py:73, shutdown in 0.5 seconds ...
[INFO] 2019-01-23 22:12:23, pid=1203, src=server.py:83, shutdown finished
[INFO] 2019-01-23 22:22:22, pid=3533, src=inference.py:57, Enable inference model done.
[INFO] 2019-01-23 22:22:22, pid=3533, src=server.py:114, server start, port=6008
[WARNING] 2019-01-23 22:28:20, pid=3533, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:28:20, pid=3533, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:28:20, pid=3533, src=server.py:73, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 22:28:20, pid=3533, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:28:20, pid=3533, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:28:20, pid=3533, src=server.py:73, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 22:28:21, pid=3533, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:28:21, pid=3533, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:28:21, pid=3533, src=server.py:73, shutdown in 0.5 seconds ...
[INFO] 2019-01-23 22:28:21, pid=3533, src=server.py:83, shutdown finished
[INFO] 2019-01-23 22:28:30, pid=4932, src=inference.py:57, Enable inference model done.
[INFO] 2019-01-23 22:28:30, pid=4932, src=server.py:114, server start, port=6008
[WARNING] 2019-01-23 22:32:35, pid=4932, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:32:35, pid=4932, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:32:35, pid=4932, src=server.py:73, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 22:32:35, pid=4932, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:32:35, pid=4932, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:32:35, pid=4932, src=server.py:73, shutdown in 0.5 seconds ...
[INFO] 2019-01-23 22:32:36, pid=4932, src=server.py:83, shutdown finished
[INFO] 2019-01-23 22:32:45, pid=6075, src=inference.py:57, Enable inference model done.
[INFO] 2019-01-23 22:32:45, pid=6075, src=server.py:114, server start, port=6008
[WARNING] 2019-01-23 22:33:09, pid=6075, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:33:09, pid=6075, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:33:09, pid=6075, src=server.py:73, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 22:33:09, pid=6075, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:33:09, pid=6075, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:33:09, pid=6075, src=server.py:73, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 22:33:10, pid=6075, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:33:10, pid=6075, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:33:10, pid=6075, src=server.py:73, shutdown in 0.5 seconds ...
[INFO] 2019-01-23 22:33:10, pid=6075, src=server.py:83, shutdown finished
[INFO] 2019-01-23 22:47:50, pid=9248, src=inference.py:57, Enable inference model done.
[INFO] 2019-01-23 22:47:50, pid=9248, src=server.py:114, server start, port=6008
[WARNING] 2019-01-23 22:49:57, pid=9248, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:49:57, pid=9248, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:49:57, pid=9248, src=server.py:73, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 22:49:57, pid=9248, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:49:57, pid=9248, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:49:57, pid=9248, src=server.py:73, shutdown in 0.5 seconds ...
[INFO] 2019-01-23 22:49:58, pid=9248, src=server.py:83, shutdown finished
[INFO] 2019-01-23 22:50:12, pid=9927, src=inference.py:57, Enable inference model done.
[INFO] 2019-01-23 22:50:12, pid=9927, src=server.py:114, server start, port=6008
[WARNING] 2019-01-23 22:53:16, pid=9927, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:53:16, pid=9927, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:53:16, pid=9927, src=server.py:73, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 22:53:17, pid=9927, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:53:17, pid=9927, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:53:17, pid=9927, src=server.py:73, shutdown in 0.5 seconds ...
[INFO] 2019-01-23 22:53:17, pid=9927, src=server.py:83, shutdown finished
[INFO] 2019-01-23 22:53:25, pid=10740, src=inference.py:57, Enable inference model done.
[INFO] 2019-01-23 22:53:25, pid=10740, src=server.py:114, server start, port=6008
[WARNING] 2019-01-23 22:55:41, pid=10740, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:55:41, pid=10740, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:55:41, pid=10740, src=server.py:73, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 22:55:41, pid=10740, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:55:41, pid=10740, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:55:41, pid=10740, src=server.py:73, shutdown in 0.5 seconds ...
[INFO] 2019-01-23 22:55:42, pid=10740, src=server.py:83, shutdown finished
[INFO] 2019-01-23 22:55:49, pid=11462, src=inference.py:57, Enable inference model done.
[INFO] 2019-01-23 22:55:49, pid=11462, src=server.py:114, server start, port=6008
[WARNING] 2019-01-23 22:56:18, pid=11462, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:56:18, pid=11462, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:56:18, pid=11462, src=server.py:73, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 22:56:19, pid=11462, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:56:19, pid=11462, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:56:19, pid=11462, src=server.py:73, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 22:56:19, pid=11462, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:56:19, pid=11462, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:56:19, pid=11462, src=server.py:73, shutdown in 0.5 seconds ...
[INFO] 2019-01-23 22:56:19, pid=11462, src=server.py:83, shutdown finished
[INFO] 2019-01-23 22:56:28, pid=11794, src=inference.py:57, Enable inference model done.
[INFO] 2019-01-23 22:56:28, pid=11794, src=server.py:114, server start, port=6008
[WARNING] 2019-01-23 22:56:45, pid=11794, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:56:45, pid=11794, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:56:45, pid=11794, src=server.py:73, shutdown in 0.5 seconds ...
[INFO] 2019-01-23 22:56:45, pid=11794, src=server.py:83, shutdown finished
[INFO] 2019-01-23 22:58:06, pid=12273, src=inference.py:57, Enable inference model done.
[INFO] 2019-01-23 22:58:06, pid=12273, src=server.py:114, server start, port=6008
[WARNING] 2019-01-23 22:58:44, pid=12273, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:58:44, pid=12273, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:58:44, pid=12273, src=server.py:73, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 22:58:44, pid=12273, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:58:44, pid=12273, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:58:44, pid=12273, src=server.py:73, shutdown in 0.5 seconds ...
[INFO] 2019-01-23 22:58:45, pid=12273, src=server.py:83, shutdown finished
[INFO] 2019-01-23 22:58:54, pid=12623, src=inference.py:57, Enable inference model done.
[INFO] 2019-01-23 22:58:54, pid=12623, src=server.py:114, server start, port=6008
[WARNING] 2019-01-23 22:59:05, pid=12623, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:59:05, pid=12623, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:59:05, pid=12623, src=server.py:73, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 22:59:05, pid=12623, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:59:05, pid=12623, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:59:05, pid=12623, src=server.py:73, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 22:59:05, pid=12623, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 22:59:05, pid=12623, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 22:59:05, pid=12623, src=server.py:73, shutdown in 0.5 seconds ...
[INFO] 2019-01-23 22:59:06, pid=12623, src=server.py:83, shutdown finished
[INFO] 2019-01-23 22:59:36, pid=12970, src=inference.py:57, Enable inference model done.
[INFO] 2019-01-23 22:59:36, pid=12970, src=server.py:114, server start, port=6008
[WARNING] 2019-01-23 23:00:51, pid=12970, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 23:00:51, pid=12970, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 23:00:51, pid=12970, src=server.py:73, shutdown in 0.5 seconds ...
[INFO] 2019-01-23 23:00:51, pid=12970, src=server.py:83, shutdown finished
[INFO] 2019-01-23 23:00:59, pid=13424, src=inference.py:57, Enable inference model done.
[INFO] 2019-01-23 23:00:59, pid=13424, src=server.py:114, server start, port=6008
[WARNING] 2019-01-23 23:01:35, pid=13424, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 23:01:35, pid=13424, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 23:01:35, pid=13424, src=server.py:73, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 23:01:35, pid=13424, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 23:01:35, pid=13424, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 23:01:35, pid=13424, src=server.py:73, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 23:01:35, pid=13424, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 23:01:35, pid=13424, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 23:01:35, pid=13424, src=server.py:73, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 23:01:35, pid=13424, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 23:01:35, pid=13424, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 23:01:35, pid=13424, src=server.py:73, shutdown in 0.5 seconds ...
[INFO] 2019-01-23 23:01:36, pid=13424, src=server.py:83, shutdown finished
[INFO] 2019-01-23 23:07:56, pid=15039, src=inference.py:57, Enable inference model done.
[INFO] 2019-01-23 23:07:56, pid=15039, src=server.py:114, server start, port=6008
[WARNING] 2019-01-23 23:08:59, pid=15039, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 23:08:59, pid=15039, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 23:08:59, pid=15039, src=server.py:73, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 23:09:00, pid=15039, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 23:09:00, pid=15039, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 23:09:00, pid=15039, src=server.py:73, shutdown in 0.5 seconds ...
[INFO] 2019-01-23 23:09:00, pid=15039, src=server.py:83, shutdown finished
[INFO] 2019-01-23 23:09:08, pid=15484, src=inference.py:57, Enable inference model done.
[INFO] 2019-01-23 23:09:08, pid=15484, src=server.py:114, server start, port=6008
[WARNING] 2019-01-23 23:10:44, pid=15484, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 23:10:44, pid=15484, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 23:10:44, pid=15484, src=server.py:73, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 23:10:44, pid=15484, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 23:10:44, pid=15484, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 23:10:44, pid=15484, src=server.py:73, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 23:10:44, pid=15484, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 23:10:44, pid=15484, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 23:10:44, pid=15484, src=server.py:73, shutdown in 0.5 seconds ...
[INFO] 2019-01-23 23:10:45, pid=15484, src=server.py:83, shutdown finished
[INFO] 2019-01-23 23:10:52, pid=15997, src=inference.py:57, Enable inference model done.
[INFO] 2019-01-23 23:10:52, pid=15997, src=server.py:114, server start, port=6008
[WARNING] 2019-01-23 23:12:35, pid=15997, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 23:12:35, pid=15997, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 23:12:35, pid=15997, src=server.py:73, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 23:12:36, pid=15997, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 23:12:36, pid=15997, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 23:12:36, pid=15997, src=server.py:73, shutdown in 0.5 seconds ...
[INFO] 2019-01-23 23:12:36, pid=15997, src=server.py:83, shutdown finished
[INFO] 2019-01-23 23:12:44, pid=16552, src=inference.py:57, Enable inference model done.
[INFO] 2019-01-23 23:12:44, pid=16552, src=server.py:114, server start, port=6008
[WARNING] 2019-01-23 23:13:21, pid=16552, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 23:13:21, pid=16552, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 23:13:21, pid=16552, src=server.py:73, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 23:13:22, pid=16552, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 23:13:22, pid=16552, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 23:13:22, pid=16552, src=server.py:73, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 23:13:22, pid=16552, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 23:13:22, pid=16552, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 23:13:22, pid=16552, src=server.py:73, shutdown in 0.5 seconds ...
[WARNING] 2019-01-23 23:13:22, pid=16552, src=server.py:65, Caught signal: 2
[INFO] 2019-01-23 23:13:22, pid=16552, src=server.py:70, begin to stop http server ...
[INFO] 2019-01-23 23:13:22, pid=16552, src=server.py:73, shutdown in 0.5 seconds ...
[INFO] 2019-01-23 23:13:22, pid=16552, src=server.py:83, shutdown finished
[INFO] 2019-01-23 23:23:51, pid=19363, src=inference.py:57, Enable inference model done.
[INFO] 2019-01-23 23:24:22, pid=19665, src=inference.py:57, Enable inference model done.
[INFO] 2019-01-23 23:24:22, pid=19665, src=server.py:115, server start, port=6008
